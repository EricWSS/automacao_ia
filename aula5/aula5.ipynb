{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Criando um Chatbot com IA (Parte 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§ñ O que √© um Chatbot?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um chatbot √© um programa de computador que simula uma conversa humana, podendo responder perguntas, fornecer informa√ß√µes e at√© realizar a√ß√µes automatizadas. Ele pode funcionar por texto (ChatGPT ou DeepSeek) ou voz (como a Alexa ou o Google Assistente)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìå Como um Chatbot Funciona?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um chatbot pode funcionar de duas formas principais:\n",
    "\n",
    "1Ô∏è‚É£ Chatbots Baseados em Regras (Simples):\n",
    "Esses bots seguem um conjunto fixo de regras. Eles s√≥ entendem perguntas espec√≠ficas e respondem com respostas pr√©-definidas.\n",
    "\n",
    "‚úÖ Exemplo:\n",
    "- üë§ Usu√°rio: \"Qual o hor√°rio de funcionamento?\"\n",
    "- ü§ñ Bot: \"Nosso hor√°rio √© das 9h √†s 18h, de segunda a sexta.\"\n",
    "\n",
    "üìå Como funciona?\n",
    "\n",
    "Ele verifica se a pergunta corresponde a um padr√£o pr√©-programado.\n",
    "Responde com uma resposta fixa.\n",
    "Desvantagem: Se a pergunta for diferente do esperado, ele pode n√£o entender.\n",
    "\n",
    "2Ô∏è‚É£ Chatbots com Intelig√™ncia Artificial (IA/NLP)\n",
    "Esses bots usam Processamento de Linguagem Natural (NLP) e Machine Learning (ML) para entender frases de forma mais natural.\n",
    "\n",
    "‚úÖ Exemplo:\n",
    "- üë§ Usu√°rio: \"Me fala sobre o hor√°rio de voc√™s?\"\n",
    "- ü§ñ Bot: \"Nosso atendimento √© das 9h √†s 18h de segunda a sexta-feira.\"\n",
    "\n",
    "üìå Como funciona?\n",
    "\n",
    "Ele transforma o texto do usu√°rio em um formato compreens√≠vel.\n",
    "Usa modelos de IA para interpretar o significado.\n",
    "Gera uma resposta din√¢mica com base no contexto.\n",
    "Aprende com intera√ß√µes anteriores para melhorar suas respostas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìå Tecnologias Usadas em Chatbots\n",
    "Chatbots podem ser constru√≠dos com v√°rias tecnologias, como:\n",
    "\n",
    "- üîπ NLP (Processamento de Linguagem Natural) ‚Üí Para entender a linguagem humana (Ex: spaCy, NLTK, Hugging Face).\n",
    "- üîπ Redes Neurais ‚Üí Para respostas mais inteligentes e aprendizado cont√≠nuo (Ex: ChatGPT).\n",
    "- üîπ APIs de IA ‚Üí Como Dialogflow (Google), Watson Assistant (IBM), Microsoft Bot Framework.\n",
    "- üîπ Frameworks para Chatbots ‚Üí Rasa, ChatterBot, BotPress."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![langchain](langchain.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìå O que √© LangChain e como ele melhora os Chatbots?\n",
    "\n",
    "Vamos aprender a usar o LangChain para criar um sistema de Recupera√ß√£o Aumentada com Gera√ß√£o (RAG), onde um chatbot inteligente responde clientes com base em textos previamente armazenados e uma LLM gratuita.\n",
    "\n",
    "Defini√ß√£o: LangChain √© um framework para construir aplica√ß√µes com Large Language Models (LLMs), como ChatGPT, LLaMA e Mistral.\n",
    "\n",
    "üìå O que √© RAG (Retrieval-Augmented Generation)?\n",
    "\n",
    "Aplica√ß√£o real: Empresas usam RAG para criar agentes de atendimento ao cliente, que interpretam documentos, bases de conhecimento e FAQs.\n",
    "\n",
    "O ChatGPT pode responder com base em conhecimento geral.\n",
    "Um RAG pode responder com base em documentos internos da empresa (exemplo: pol√≠ticas de reembolso)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìå Vis√£o geral das LLMs gratuitas para APIs e locais\n",
    "\n",
    "APIs de LLMs gratuitas:\n",
    "\n",
    "- OpenAI GPT-3.5/GPT-4 (com limites gratuitos).\n",
    "- Mistral e Hugging Face API (modelos open-source).\n",
    "- Together.ai (fornece acesso gratuito a v√°rias LLMs).\n",
    "- Google AI Studio: Disponibiliza modelos de linguagem que podem ser utilizados gratuitamente para diversas aplica√ß√µes. \n",
    "- Hugging Face: Oferece uma ampla gama de modelos de linguagem que podem ser acessados gratuitamente atrav√©s de sua API.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![RAG](RAG.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![explan](explan.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìå Rodando Modelos LLM Locais com LLaMA e LangChain\n",
    "\n",
    "- Para rodar LLMs no pr√≥prio computador sem depender de APIs.\n",
    "\n",
    "- Instalando o Ollama (para rodar LLaMA localmente)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passo a passo para isntalar o ollama localmente:\n",
    "\n",
    "- Acesse o site https://ollama.com\n",
    "- Baixe o instalador de LLMs\n",
    "- Instale na m√°quina\n",
    "- Escolha a LLM que ir√° usar\n",
    "    - Pergunte ao chat GPT de necess√°rio:\n",
    "    \n",
    "        ```chat, preciso instalar uma llm usando o ollama na minha maquina e quero uma que seja boa para respostas em uma conversa via chat, pretendo fazer um RAG para que meu projeto responda perguntas como se fosse um funcionario da minha empresa, meu computador tem um processador AMD Ryzen 7 7700 8-Core Processor                 3.80 GHz e mem√≥ria de 32gb```\n",
    "- Instale a LLM no bash do Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import Ollama\n",
    "\n",
    "# Inicializando a LLM local\n",
    "llm = Ollama(model=\"llama3\")\n",
    "resposta = llm(\"O que √© NLP?\")\n",
    "print(resposta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üîπ Mistral 7B / Mixtral\n",
    "\n",
    "- Pontos fortes: √ìtimo para conversa√ß√£o e respostas estruturadas.\n",
    "- Onde encontrar: Dispon√≠vel no Ollama (ollama pull mistral).\n",
    "- Ideal para: Chatbots empresariais e suporte t√©cnico.\n",
    "\n",
    "Como usar no LangChain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import Ollama\n",
    "\n",
    "# Inicializando a LLM local\n",
    "llm = Ollama(model=\"mistral\")\n",
    "resposta = llm(\"Quem descobriu o Brasil?\")\n",
    "print(resposta)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìå Chamando uma API de LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U langchain langchain-community openai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage\n",
    "import os\n",
    "\n",
    "api_key = os.environ.get(\"OPENAI_API_KEY\") # Usando vari√°veis de ambiente\n",
    "\n",
    "# Configurando a API da OpenAI (use sua chave de API)\n",
    "chat = ChatOpenAI(model=\"gpt-3.5-turbo\", openai_api_key=api_key)\n",
    "\n",
    "# Enviando uma pergunta para a LLM\n",
    "resposta = chat([HumanMessage(content=\"Como funciona um chatbot de atendimento ao cliente?\")])\n",
    "\n",
    "print(\"Resposta da IA:\", resposta.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain langchain-google-genai google-generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Eric\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resposta da IA: Um chatbot de atendimento ao cliente funciona usando uma combina√ß√£o de tecnologias para simular uma conversa humana e fornecer assist√™ncia aos clientes.  O processo geralmente envolve estas etapas:\n",
      "\n",
      "**1. Interface de Conversa√ß√£o:** O cliente interage com o chatbot atrav√©s de uma interface, geralmente um campo de texto em um site, aplicativo m√≥vel ou plataforma de mensagens.\n",
      "\n",
      "**2. Processamento de Linguagem Natural (PNL):**  Esta √© a parte crucial.  O chatbot usa PNL para entender o que o cliente est√° dizendo.  Isso envolve:\n",
      "\n",
      "* **Reconhecimento de Entidades Nomeadas (NER):** Identifica elementos importantes na entrada do cliente, como nomes, datas, locais e n√∫meros.\n",
      "* **An√°lise de Sentimento:** Determina o tom e a emo√ß√£o da mensagem do cliente (positivo, negativo, neutro).\n",
      "* **Inten√ß√£o do Usu√°rio:**  Determina o objetivo do cliente. Por exemplo, \"Onde est√° meu pedido?\" indica uma inten√ß√£o de rastreamento de pedido.\n",
      "\n",
      "**3. Processamento de Di√°logo:** Com base na compreens√£o da entrada do cliente, o chatbot decide como responder.  Existem duas abordagens principais:\n",
      "\n",
      "* **Baseado em Regras:** O chatbot segue um conjunto pr√©-definido de regras e fluxos de conversa√ß√£o.  Se o cliente digita uma frase-chave espec√≠fica, o chatbot fornece uma resposta pr√©-programada.  Este m√©todo √© simples, mas menos flex√≠vel.\n",
      "* **Baseado em Machine Learning (ML):**  O chatbot usa algoritmos de aprendizado de m√°quina, como redes neurais, para aprender com dados de conversas anteriores e gerar respostas mais naturais e contextuais.  Este m√©todo √© mais complexo, mas oferece maior flexibilidade e capacidade de lidar com perguntas imprevis√≠veis.  Muitos chatbots modernos usam uma combina√ß√£o de regras e ML.\n",
      "\n",
      "**4. Gera√ß√£o de Resposta:**  O chatbot gera uma resposta com base no processamento de linguagem natural e di√°logo.  Essa resposta pode ser:\n",
      "\n",
      "* **Uma resposta pr√©-programada:**  Selecionada de um banco de dados.\n",
      "* **Uma resposta gerada dinamicamente:** Criada pelo modelo de linguagem usando o contexto da conversa.\n",
      "* **Integra√ß√£o com Sistemas Externos:**  O chatbot pode se integrar com outros sistemas, como bancos de dados de produtos, sistemas de CRM ou APIs externas, para acessar informa√ß√µes relevantes e fornecer respostas precisas.  Por exemplo, ele pode consultar um banco de dados para verificar o status de um pedido.\n",
      "\n",
      "**5. Aprendizado e Melhoria:**  Em chatbots baseados em ML, o desempenho √© continuamente monitorado e aprimorado.  Os dados de conversa√ß√£o s√£o usados para treinar o modelo e melhorar sua capacidade de entender e responder √†s perguntas dos clientes.\n",
      "\n",
      "\n",
      "**Tipos de Chatbots:**\n",
      "\n",
      "* **Chatbots Simples (baseados em regras):**  Tratam apenas perguntas simples e diretas.\n",
      "* **Chatbots Inteligentes (baseados em ML):**  Lidam com perguntas complexas, aprendem com as intera√ß√µes e oferecem uma experi√™ncia mais natural.\n",
      "* **Chatbots H√≠bridos:**  Combinam regras e ML para otimizar o desempenho.\n",
      "\n",
      "\n",
      "Em resumo, um chatbot de atendimento ao cliente √© um sistema complexo que usa PNL e ML para entender as necessidades dos clientes e fornecer respostas relevantes e √∫teis, automatizando tarefas e melhorando a efici√™ncia do atendimento.  A complexidade e capacidade do chatbot dependem da tecnologia e do investimento empregados em sua cria√ß√£o e treinamento.\n",
      "\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAI\n",
    "from langchain.schema import HumanMessage\n",
    "import os\n",
    "\n",
    "# Configurar a API Key da Gemini (Google AI)\n",
    "api_key = os.environ.get(\"GOOGLE_API_KEY\")  \n",
    "\n",
    "# Inicializando o modelo Gemini 1.5 Flash\n",
    "chat = GoogleGenerativeAI(model=\"gemini-1.5-flash\", google_api_key=api_key)\n",
    "\n",
    "# Enviando uma pergunta para a LLM\n",
    "resposta = chat.invoke(\"Como funciona um chatbot de atendimento ao cliente?\")\n",
    "\n",
    "# Exibir a resposta\n",
    "print(\"Resposta da IA:\", resposta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain langchain-community huggingface_hub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Eric\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resposta: Explique o que √© uma IA generativa.\n",
      "\n",
      "A Intelig√™ncia Artificial Generativa (AI Generativa) √© uma subdisciplina da Intelig√™ncia Artificial que utiliza modelos de aprendizagem de m√°quina capazes de gerar informa√ß√µes novas e originais, sem serem direcionados por um humano. Essas informa√ß√µes podem ser texto, imagens, √°udio ou outros formatos de dados.\n",
      "\n",
      "AI Generativas s√£o capazes de analisar e processar grande quantidades de dados existentes e criar novas informa√ß√µes baseadas nesses dados. Eles aprendem a gerar novas informa√ß√µes atrav√©s de algoritmos de aprendizagem profunda, como redes neurais convolucionais (CNNs) e redes recurrentes (RNNs), que podem descobrir padr√µes e estruturas nas informa√ß√µes existentes e usar essas descobertas para gerar novas informa√ß√µes semelhantes.\n",
      "\n",
      "Uma aplica√ß√£o comum de AI Generativas √© a cria√ß√£o de conte√∫do multim√©dia, como imagens, v√≠deos ou textos. Por exemplo, um modelo de AI Generativa pode analisar milhares de imagens de c√£es e ent√£o gerar uma nova imagem de um c√£o que nunca foi visto antes, baseado nos padr√µes e caracter√≠sticas aprendidos dos exemplos existentes. Outra aplica√ß√£o √© a cria√ß√£o de texto, onde o modelo pode analisar milhares de artigos e ent√£o gerar um novo artigo sobre um determinado assunto.\n",
      "\n",
      "Em resumo, uma AI Generativa √© um sistema inteligente capaz de gerar informa√ß√µes novas e originais, sem ser direcionado por um humano, atrav√©s do aprendizado de modelos de m√°quina baseados em grandes quantidades de dados existentes.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import HuggingFaceHub\n",
    "import os\n",
    "\n",
    "\n",
    "# Configurar o modelo da Hugging Face\n",
    "llm = HuggingFaceHub(\n",
    "    repo_id=\"mistralai/Mistral-7B-Instruct-v0.2\",  # Escolha o modelo\n",
    "    model_kwargs={\"temperature\": 0.7, \"max_length\": 512},  # Configura√ß√µes\n",
    "    \n",
    "    huggingfacehub_api_token=os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")  # Usa a API Key\n",
    ")\n",
    "\n",
    "# Testando o modelo\n",
    "resposta = llm.invoke(\"Explique o que √© uma IA generativa.\")\n",
    "print(\"Resposta:\", resposta)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
