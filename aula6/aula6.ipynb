{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìå Rodando Modelos LLM Locais com LLaMA e LangChain\n",
    "\n",
    "- Para rodar LLMs no pr√≥prio computador sem depender de APIs.\n",
    "\n",
    "- Instalando o Ollama (para rodar LLaMA localmente)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passo a passo para isntalar o ollama localmente:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Acesse o site https://ollama.com\n",
    "- Baixe o instalador de LLMs\n",
    "- Instale na m√°quina\n",
    "- Escolha a LLM que ir√° usar\n",
    "- Instale a LLM no bash do Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain langchain-community"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üîπ Mistral 7B / Mixtral\n",
    "\n",
    "- Pontos fortes: √ìtimo para conversa√ß√£o e respostas estruturadas.\n",
    "- Onde encontrar: Dispon√≠vel no Ollama (ollama pull mistral).\n",
    "- Ideal para: Chatbots empresariais e suporte t√©cnico.\n",
    "\n",
    "Como usar no LangChain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import Ollama\n",
    "\n",
    "# Inicializando a LLM local\n",
    "llm = Ollama(model=\"mistral\")\n",
    "resposta = llm(\"Quem descobriu o Brasil?\")\n",
    "print(resposta)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üîπ LLaMA 3 (Meta)\n",
    "\n",
    "- Pontos fortes: √ìtima compreens√£o de contexto e menor consumo de mem√≥ria.\n",
    "- Onde encontrar: Dispon√≠vel via Ollama (ollama pull llama3).\n",
    "\n",
    "Como usar no LangChain:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import Ollama\n",
    "\n",
    "# Inicializando a LLM local\n",
    "llm = Ollama(model=\"llama3\")\n",
    "resposta = llm(\"O que √© NLP?\")\n",
    "print(resposta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìå Como Configurar um Chatbot Estruturado com RAG no LangChain\n",
    "\n",
    "Agora, vamos configurar um pipeline de RAG para seu chatbot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passo 1: Criando a Base de Conhecimento\n",
    "\n",
    "Agora, vamos configurar um pipeline de RAG para seu chatbot.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain.schema import Document\n",
    "\n",
    "# Criando documentos para o FAISS\n",
    "documentos = [\n",
    "    Document(page_content=\"Nosso suporte est√° dispon√≠vel de segunda a sexta, das 9h √†s 18h.\"),\n",
    "    Document(page_content=\"O reembolso pode ser solicitado em at√© 7 dias ap√≥s a compra.\"),\n",
    "    Document(page_content=\"Para falar com um atendente, envie um e-mail para suporte@empresa.com.\"),\n",
    "]\n",
    "\n",
    "# Usando embeddings locais com Ollama (modelo Mistral)\n",
    "vectorstore = FAISS.from_documents(documentos, OllamaEmbeddings(model=\"mistral\"))\n",
    "\n",
    "print(\"Banco de vetores criado com sucesso usando Mistral!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úÖ Isso cria um banco de vetores com informa√ß√µes do suporte ao cliente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passo 2: Criando o Chatbot RAG\n",
    "\n",
    "Agora, criamos um agente que busca respostas antes de responder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain_community.llms import Ollama  # Import correto para Ollama\n",
    "\n",
    "llm = Ollama(model=\"mistral\")\n",
    "\n",
    "# Criando a cadeia de perguntas e respostas com recupera√ß√£o de contexto\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=vectorstore.as_retriever()\n",
    ")\n",
    "\n",
    "# Fazendo uma pergunta ao chatbot\n",
    "pergunta = \"Qual o hor√°rio de atendimento?\"\n",
    "resposta = qa.run(pergunta)\n",
    "\n",
    "print(\"Resposta do Chatbot:\", resposta)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclus√£o\n",
    "\n",
    "Qual LLM Escolher?\n",
    "\n",
    "![taela_llm](taela_llm.png)\n",
    "\n",
    "- ‚úÖ Se precisar de um chatbot local, v√° de Mistral 7B ou LLaMA 3.\n",
    "- ‚úÖ Se precisar de alta precis√£o, use GPT-4 Turbo ou Cohere API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "proxima aula chroma - vetorizador"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
